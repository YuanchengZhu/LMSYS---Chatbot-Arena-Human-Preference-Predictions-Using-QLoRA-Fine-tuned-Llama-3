{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMSYS - Chatbot Arena Human Preference Predictions Using QLoRA Fine-tuned Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the notebooks I created for Kaggle competition: [LMSYS - Chatbot Arena Human Preference Predictions](https://www.kaggle.com/competitions/lmsys-chatbot-arena). \n",
    "\n",
    "Basically, this competition challenges you to predict which responses users will prefer (or a tie) in a head-to-head battle between chatbots powered by large language models (LLMs). You'll be given a dataset of conversations from the Chatbot Arena, where different LLMs generate answers to user prompts. I didn't formally attend this competition, but I think it is a good opportunity to gain practical experience in LLM fine-tuning as the dataset mainly consists of text data: question, response of model a and response of model b.\n",
    "\n",
    "I used Llama 3 - 8B as the basic model for classification and fine-tuned the model using QLoRA. QLoRA, stands for, Quantized Low Rank Adaptation, which is a parameter efficient fine tuning method. Since I was fine tuning on a consumer-level GPU (RTX 4090) with my own desktop, I used the smallest model (8B) and leveraged quantization and low rank adaption to save memory usage and maintain an accpetable training speed.\n",
    "\n",
    "I tried different combination of training parameters and this notebook includes the latest version. It seems that the model still has improvement space. However, to restart the training with new parameters will take a lot of time. I am considering combining the base model and adapter as the new base model and do fine-tuning on the new base model. However, currently, when trying to do this, it seems my GPU is not working. I need to do more research on how to realize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v7'\n",
    "\n",
    "RESULT_PATH = './' + VERSION + '_results/' \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "\n",
    "PATH = '/kaggle/input/lmsys-chatbot-arena/'\n",
    "PATH = 'C:/Users/zyc71/Data Science Projects/LMSYS - Chatbot Arena Human Preference Predictions/Data/'\n",
    "\n",
    "PERSONAL_PATH = 'C:/Users/zyc71/Data Science Projects/LMSYS - Chatbot Arena Human Preference Predictions/'\n",
    "\n",
    "TRAIN_CSV = PATH + 'train.csv'\n",
    "TEST_CSV = PATH + 'test.csv'\n",
    "SUBM_CSV = PATH + 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "def preprocess_text(df, cols = ['prompt', 'response_a', 'response_b']):\n",
    "    for col in cols:\n",
    "        # Remove the [\" and \"] that appear at the beginning and the end of text.\n",
    "        df[col] = df[col].str.replace(pat = r'^(\\[\")', repl = '', regex = True).str.replace(pat = r'(\"\\])$', repl = '', regex = True)\n",
    "    \n",
    "    return df\n",
    "        \n",
    "# Caculate length of response\n",
    "def calculate_length(df):\n",
    "    df['len_prompt'] = df['prompt'].str.len()\n",
    "    df['len_a'] = df['response_a'].str.len()\n",
    "    df['len_b'] = df['response_b'].str.len()\n",
    "    df['len_diff'] = df['len_a'] - df['len_b']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Count the tokens of response\n",
    "def count_tokens(df):\n",
    "    df['token_num_prompt'] = df['prompt'].apply(lambda x: len(word_tokenize(x)))\n",
    "    df['token_num_a'] = df['response_a'].apply(lambda x: len(word_tokenize(x)))\n",
    "    df['token_num_b'] = df['response_b'].apply(lambda x: len(word_tokenize(x)))\n",
    "    df['token_num_diff'] = df['token_num_a'] - df['token_num_b']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Count the sentenses of response\n",
    "def count_sentenses(df):\n",
    "    df['sentense_num_prompt'] = df['prompt'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['sentense_num_a'] = df['response_a'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['sentense_num_b'] = df['response_b'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['sentense_num_diff'] = df['sentense_num_a'] - df['sentense_num_b']\n",
    "    \n",
    "    return df\n",
    "\n",
    " # Create prompts for prediction use\n",
    "def create_prompt(df):\n",
    "    df['prompt_for_pred'] = ('Based on user prompt, please find out whether user likes response of Model A or response of Model B or think they are tied.\\n\\n' \n",
    "    + 'User prompt:\\n' + df['prompt'] + '\\n\\n' + 'Model A:\\n' + df['response_a'] + '\\n\\n' + 'Model B:\\n' + df['response_b'])\n",
    "     \n",
    "    return df\n",
    "\n",
    "# Create unified target column - 'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2\n",
    "# NOTICE: This is only applicable to train data\n",
    "def create_label_column(df):\n",
    "    df['label'] = df.apply(lambda row: 0 if row['winner_model_a'] == 1 else 1 if row['winner_model_b'] == 1 else 2, axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Pipe all the processing functions\n",
    "def pipe_processing_function(df):\n",
    "    df = (\n",
    "        df.pipe(preprocess_text)\n",
    "        .pipe(calculate_length)\n",
    "        .pipe(count_tokens)\n",
    "        .pipe(count_sentenses)\n",
    "        .pipe(create_prompt)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_train = pipe_processing_function(df_train)\n",
    "# df_train = create_label_column(df_train)\n",
    "# df_train.to_csv(PATH + 'processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data to save time\n",
    "df_train = pd.read_csv(PATH + 'processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>...</th>\n",
       "      <th>token_num_prompt</th>\n",
       "      <th>token_num_a</th>\n",
       "      <th>token_num_b</th>\n",
       "      <th>token_num_diff</th>\n",
       "      <th>sentense_num_prompt</th>\n",
       "      <th>sentense_num_a</th>\n",
       "      <th>sentense_num_b</th>\n",
       "      <th>sentense_num_diff</th>\n",
       "      <th>prompt_for_pred</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>827</td>\n",
       "      <td>241</td>\n",
       "      <td>586</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>Based on user prompt, please find out whether ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>591</td>\n",
       "      <td>655</td>\n",
       "      <td>-64</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>-5</td>\n",
       "      <td>Based on user prompt, please find out whether ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>212</td>\n",
       "      <td>423</td>\n",
       "      <td>-211</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>-6</td>\n",
       "      <td>Based on user prompt, please find out whether ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id             model_a         model_b  \\\n",
       "0           0  30192  gpt-4-1106-preview      gpt-4-0613   \n",
       "1           1  53567           koala-13b      gpt-4-0613   \n",
       "2           2  65089  gpt-3.5-turbo-0613  mistral-medium   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  A marriage license and a marriage certificate ...               0   \n",
       "2  Function calling is the process of invoking a ...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  ...  token_num_prompt  token_num_a  \\\n",
       "0               0           0  ...                36          827   \n",
       "1               1           0  ...                45          591   \n",
       "2               0           1  ...                11          212   \n",
       "\n",
       "   token_num_b  token_num_diff  sentense_num_prompt  sentense_num_a  \\\n",
       "0          241             586                    3              26   \n",
       "1          655             -64                    3              15   \n",
       "2          423            -211                    2               5   \n",
       "\n",
       "   sentense_num_b  sentense_num_diff  \\\n",
       "0              12                 14   \n",
       "1              20                 -5   \n",
       "2              11                 -6   \n",
       "\n",
       "                                     prompt_for_pred  label  \n",
       "0  Based on user prompt, please find out whether ...      0  \n",
       "1  Based on user prompt, please find out whether ...      1  \n",
       "2  Based on user prompt, please find out whether ...      2  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Fine Tuning Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LlamaForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\zyc71\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# To use Llama3, we need to register for an access token\n",
    "# access_token = 'Your Llama Token Here.'\n",
    "from huggingface_hub import login\n",
    "login(token = access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "# model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "# model_name = 'mistralai/Mistral-7B-v0.3'\n",
    "\n",
    "# If GPU is available, device = 'auto' will proritize the use of GPU.\n",
    "device = 'auto'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_auth_token = True, # The token to use as HTTP bearer authorization for remote files. Passing use_auth_token=True is required when you want to use a private model.\n",
    "    use_fast = True, # load the fast version of the tokenizer\n",
    "    padding_side = 'left',\n",
    "    truncation_side = 'right',\n",
    ")\n",
    "\n",
    "\n",
    "# Add a self defined padding token\n",
    "# Note: Llama has no pad and unknown token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "tokenizer.pad_token = tokenizer.pad_token\n",
    "\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch['prompt_for_pred'],\n",
    "                     padding = 'max_length',\n",
    "                     max_length = 1024,\n",
    "                     truncation = True,\n",
    "                     return_tensors = 'pt', # return pytorch tensor\n",
    "                     )\n",
    "\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff16b38b83f4ace9cfadc1555136df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantization configuration to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # Quantize the model to 4-bits when load it\n",
    "    bnb_4bit_quant_type = 'nf4', # Use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16, # Use bfloat16 for faster computation\n",
    "    bnb_4bit_use_double_quant = True, # Use a nested quantization scheme to quantize the already quantized weights\n",
    ")\n",
    "\n",
    "\n",
    "# Use quantization method load Llama3 for classification\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels = 3, # We have 3 classes: model a wins, model b wins and tie\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = device,\n",
    "        trust_remote_code = True\n",
    ")\n",
    "\n",
    "\n",
    "# Gradient checkpointing is a technique used to trade off memory usage for computation time during backpropagation\n",
    "# When activated, it is used to reduce memory consumption by saving only certain intermediate activations during the forward pass \n",
    "# and recomputing others during the backward pass. This trades off increased computation time for reduced memory usage.\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# what prepare_mode_for_kbit_training() does? - https://anelmusic13.medium.com/turn-your-llm-into-a-mafioso-code-explanation-companion-8cef7dfee80a\n",
    "# use_gradient_checkpointing is True by default in prepare_model_for_kbit_training()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# Since I create a pad token for fine-tuning, some changes are needed\n",
    "# To avoid ValueError: Cannot handle batch sizes > 1 if no padding token is defined.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Resize token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128257, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=4096, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA cofiguration\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 4, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 16, # scaling factor to control the influence of the LoRA relative to the original model weights. A higher lora_alpha gives more weight to the low-rank updates\n",
    "    lora_dropout = 0.1, # dropout probability of the LoRA layers\n",
    "    bias = 'none', \n",
    "    task_type = 'SEQ_CLS', # Need to set 'SEQ_CLS' for classification\n",
    "    target_modules = ['q_proj', 'v_proj']\n",
    "    # target_modules=[\"q_proj\",\n",
    "    #     \"k_proj\",\n",
    "    #     \"v_proj\",\n",
    "    #     \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "        # \"lm_head\",\n",
    "                   # ]\n",
    ")\n",
    "\n",
    "# Once the LoraConfig is setup, create a PeftModel with the get_peft_model() function. \n",
    "# It takes a base model - which you can load from the Transformers library - and the LoraConfig containing the parameters for how to configure a model for training with LoRA.\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,716,224 || all params: 7,506,657,280 || trainable%: 0.0229\n"
     ]
    }
   ],
   "source": [
    "# print_trainable_parameters() is a method that can be used after get_peft_model()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in ['winner_model_a', 'winner_model_b', 'winner_tie', 'label']]\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df_train[feature_cols], df_train['label'], test_size = 0.05, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.1, random_state = 42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(pd.concat([X_train[['prompt_for_pred']], y_train], axis = 1))\n",
    "val_dataset = Dataset.from_pandas(pd.concat([X_val[['prompt_for_pred']], y_val], axis = 1))\n",
    "test_dataset = Dataset.from_pandas(pd.concat([X_test[['prompt_for_pred']], y_test], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0bd1cd4f174d809f687eaff36bb50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c16e2ff9804ac0bb5c5b2ff4242fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the prompt using pre-defined function\n",
    "train_ds = train_dataset.map(tokenize_text, batched = True)\n",
    "val_ds = val_dataset.map(tokenize_text, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    pred_probas = pred.predictions\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    macro_f1_score = f1_score(labels, preds, average='macro')\n",
    "    log_loss_score = log_loss(y_true = labels, y_pred = pred_probas)\n",
    "    return {'log_loss_score': log_loss_score, 'accuracy': accuracy, 'macro_f1_score': macro_f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For test use\n",
    "# train_ds = train_ds.select(range(5000))\n",
    "# val_ds = val_ds.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameter configuration\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "# Gradient accumulation is a way to virtually increase the batch size during training\n",
    "'''\n",
    "For example, if you want to use a batch size of 256 but can only fit a batch size of 64 into GPU memory, you can perform gradient accumulation \n",
    "over four batches of size 64. That is, set batch_size = 64 and gradient_accumulation_steps = 4\n",
    "'''\n",
    "gradient_accumulation_steps = 6\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = RESULT_PATH,\n",
    "    logging_dir = RESULT_PATH + 'logs',\n",
    "    \n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "    \n",
    "    learning_rate = 2e-4, # The initial learning rate for Adam.\n",
    "    # warmup_ratio = 0.01, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\n",
    "    weight_decay = 0.05, # Regularization parameter for L2 penalty\n",
    "    \n",
    "    lr_scheduler_type = 'polynomial',\n",
    "    fp16 = True, # Mixed precision training can speed up the computations by reducing some variables to fp16 instead of keeping all variables in fp32\n",
    "    optim = 'paged_adamw_32bit', # The optimizer to use for training the model, 'paged_adamw_32bit' is a variant of the AdamW optimizer designed to be more efficient on 32-bit GPUs.\n",
    "\n",
    "    logging_strategy = 'steps',\n",
    "    eval_strategy = 'steps',\n",
    "    save_strategy = 'steps',\n",
    "    logging_steps = 0.07, # Number of update steps between two logs\n",
    "    eval_steps = 0.07, # Number of update steps between two evaluations\n",
    "    save_steps = 0.07, # Number of updates steps before two checkpoint saves\n",
    "    \n",
    "    load_best_model_at_end = True, # Whether or not to load the best model found during training at the end of training\n",
    "    metric_for_best_model = 'eval_loss',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24570' max='24570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24570/24570 73:07:14, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Log Loss Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.347300</td>\n",
       "      <td>1.229279</td>\n",
       "      <td>3.686457</td>\n",
       "      <td>0.412012</td>\n",
       "      <td>0.395194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.139500</td>\n",
       "      <td>1.195851</td>\n",
       "      <td>3.383888</td>\n",
       "      <td>0.404138</td>\n",
       "      <td>0.398865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>1.089600</td>\n",
       "      <td>1.171210</td>\n",
       "      <td>3.345467</td>\n",
       "      <td>0.420985</td>\n",
       "      <td>0.405652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6880</td>\n",
       "      <td>1.059900</td>\n",
       "      <td>1.154451</td>\n",
       "      <td>3.169676</td>\n",
       "      <td>0.424831</td>\n",
       "      <td>0.416609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.044800</td>\n",
       "      <td>1.160874</td>\n",
       "      <td>3.276569</td>\n",
       "      <td>0.418238</td>\n",
       "      <td>0.416523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10320</td>\n",
       "      <td>1.032100</td>\n",
       "      <td>1.163804</td>\n",
       "      <td>3.404346</td>\n",
       "      <td>0.427211</td>\n",
       "      <td>0.409752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12040</td>\n",
       "      <td>1.022500</td>\n",
       "      <td>1.159704</td>\n",
       "      <td>3.374507</td>\n",
       "      <td>0.428493</td>\n",
       "      <td>0.416291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13760</td>\n",
       "      <td>1.010700</td>\n",
       "      <td>1.149961</td>\n",
       "      <td>3.200396</td>\n",
       "      <td>0.414393</td>\n",
       "      <td>0.413192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15480</td>\n",
       "      <td>1.002600</td>\n",
       "      <td>1.152832</td>\n",
       "      <td>3.377131</td>\n",
       "      <td>0.425563</td>\n",
       "      <td>0.414514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.995500</td>\n",
       "      <td>1.150648</td>\n",
       "      <td>3.327080</td>\n",
       "      <td>0.426112</td>\n",
       "      <td>0.410535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18920</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>1.136274</td>\n",
       "      <td>3.147010</td>\n",
       "      <td>0.437466</td>\n",
       "      <td>0.429355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20640</td>\n",
       "      <td>0.979800</td>\n",
       "      <td>1.135399</td>\n",
       "      <td>3.112170</td>\n",
       "      <td>0.425929</td>\n",
       "      <td>0.423577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22360</td>\n",
       "      <td>0.972100</td>\n",
       "      <td>1.134570</td>\n",
       "      <td>3.150930</td>\n",
       "      <td>0.426662</td>\n",
       "      <td>0.422174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24080</td>\n",
       "      <td>0.966800</td>\n",
       "      <td>1.131300</td>\n",
       "      <td>3.114278</td>\n",
       "      <td>0.427760</td>\n",
       "      <td>0.424682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10h 13min 34s\n",
      "Wall time: 3d 1h 7min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24570, training_loss=1.0446081152959814, metrics={'train_runtime': 263245.0468, 'train_samples_per_second': 5.6, 'train_steps_per_second': 0.093, 'total_flos': 6.322321783640398e+19, 'train_loss': 1.0446081152959814, 'epoch': 29.99389623601221})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# DataCollatorWithPadding handles cases where input sequences have different lengths by dynamically padding them within a batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "# For DataCollatorWithPadding, the default parameters are: padding = True, max_length = None, return_tensors = 'pt' \n",
    "# In fact, if we use DataCollatorWithPadding, we don't need to pad the input sequence originally.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = val_ds,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer = tokenizer, padding = 'max_length', max_length = 1024, return_tensors = 'pt'),\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./v7_results/tokenizer\\\\tokenizer_config.json',\n",
       " './v7_results/tokenizer\\\\special_tokens_map.json',\n",
       " './v7_results/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "trainer.save_model(RESULT_PATH +'model')\n",
    "\n",
    "tokenizer.save_pretrained(RESULT_PATH + 'tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In inference part, I duplicates some codes above so that I can run inference part directly after the model is fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LlamaForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v7'\n",
    "\n",
    "RESULT_PATH = './' + VERSION + '_results/' \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "\n",
    "PATH = '/kaggle/input/lmsys-chatbot-arena/'\n",
    "PATH = 'C:/Users/zyc71/Data Science Projects/LMSYS - Chatbot Arena Human Preference Predictions/Data/'\n",
    "\n",
    "PERSONAL_PATH = 'C:/Users/zyc71/Data Science Projects/LMSYS - Chatbot Arena Human Preference Predictions/'\n",
    "\n",
    "TRAIN_CSV = PATH + 'train.csv'\n",
    "TEST_CSV = PATH + 'test.csv'\n",
    "SUBM_CSV = PATH + 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\zyc71\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# access_token = 'Your Llama Token Here.'\n",
    "from huggingface_hub import login\n",
    "login(token = access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='auto'\n",
    "\n",
    "# base model is what I fine tuned\n",
    "base_model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "# Below is where I save the fine tuned adapter\n",
    "model_name = RESULT_PATH + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load saved tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(RESULT_PATH + 'tokenizer')\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch['prompt_for_pred'],\n",
    "                     padding = 'max_length',\n",
    "                     max_length = 1024,\n",
    "                     truncation = True,\n",
    "                     return_tensors = 'pt',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2369610890c40dfaf91cac7fbfb6697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model with quantization\n",
    "\n",
    "# Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "\n",
    "#Load the base model with default precision\n",
    "model = LlamaForSequenceClassification.from_pretrained(base_model_name, \n",
    "                                                       num_labels = 3, \n",
    "                                                       quantization_config = bnb_config, \n",
    "                                                       device_map = device, \n",
    "                                                       trust_remote_code = True,\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To avoid ValueError: Cannot handle batch sizes > 1 if no padding token is defined.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Resize token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and activate the adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyc71\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Merge the adapter with the base model\n",
    "'''\n",
    "While LoRA is significantly smaller and faster to train, you may encounter latency issues during inference due to separately loading the base model \n",
    "and the LoRA adapter. To eliminate latency, use the merge_and_unload() function to merge the adapter weights with the base model. This allows you to \n",
    "use the newly merged model as a standalone model. The merge_and_unload() function doesnâ€™t keep the adapter weights in memory.\n",
    "'''\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data to save time\n",
    "df_train = pd.read_csv(PATH + 'processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in ['winner_model_a', 'winner_model_b', 'winner_tie', 'label']]\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df_train[feature_cols], df_train['label'], test_size = 0.1, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2, random_state = 42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(pd.concat([X_train[['prompt_for_pred']], y_train], axis = 1))\n",
    "val_dataset = Dataset.from_pandas(pd.concat([X_val[['prompt_for_pred']], y_val], axis = 1))\n",
    "test_dataset = Dataset.from_pandas(pd.concat([X_test[['prompt_for_pred']], y_test], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a554b4124448e59562e6d8e030a591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the prompt using pre-defined function\n",
    "# train_ds = train_dataset.map(tokenize_text, batched = True)\n",
    "# val_ds = val_dataset.map(tokenize_text, batched = True)\n",
    "\n",
    "infer_ds = test_dataset.select(range(0,1000)).map(tokenize_text, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:59<00:00,  5.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5650298 , 0.16093798, 0.2740322 ],\n",
       "       [0.13291879, 0.61656404, 0.25051716],\n",
       "       [0.2341742 , 0.3793606 , 0.38646522],\n",
       "       ...,\n",
       "       [0.17246105, 0.62455326, 0.20298564],\n",
       "       [0.51112574, 0.36297372, 0.12590058],\n",
       "       [0.2586679 , 0.5066017 , 0.23473048]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predicted = []\n",
    "\n",
    "for text in tqdm(infer_ds['prompt_for_pred']):\n",
    "    # Tokenize the text and create a batch with a single data point\n",
    "    tokenized = tokenizer(text, return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 1024)\n",
    "\n",
    "    # Perform inference on the single data point\n",
    "    output = model(**tokenized)\n",
    "    logits = output.logits\n",
    "    logits = logits.float()\n",
    "\n",
    "    # Calculate class probabilities\n",
    "    class_probabilities = torch.nn.functional.softmax(logits, dim = 1)\n",
    "\n",
    "    predicted.append(class_probabilities)\n",
    "    \n",
    "concatenated_tensor = torch.cat(predicted)\n",
    "predicted = concatenated_tensor.detach().cpu().numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1_Score': 0.4448254396328788, 'Accuracy': 0.448, 'Log_Loss': 1.1355878386265164}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, log_loss\n",
    "\n",
    "def get_classification_report(p, y):\n",
    "    probabilities = p\n",
    "\n",
    "    labels = np.array(y)\n",
    "\n",
    "    # Threshold probabilities if needed\n",
    "    thresholded_predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "    f1 = f1_score(labels, thresholded_predictions, average='macro')\n",
    "    accuracy = accuracy_score(labels, thresholded_predictions)\n",
    "    logloss = log_loss(labels, probabilities)\n",
    "    \n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(labels, thresholded_predictions)\n",
    "    \n",
    "    # # Plot confusion matrix\n",
    "    # sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Hate', 'Hate'], yticklabels=['Non-Hate', 'Hate'])\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('True')\n",
    "    # plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    print({\"F1_Score\": f1, 'Accuracy': accuracy,\"Log_Loss\": logloss})\n",
    "\n",
    "metrics = get_classification_report(predicted, infer_ds['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_cache() is usually not working, restart the kernel and use the code in next cell to test whether GPU memory usage has been reset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 13.258203GB\n",
      "torch.cuda.memory_reserved: 13.816406GB\n",
      "torch.cuda.max_memory_reserved: 13.824219GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model still has improvement space. However, to restart the training with new parameters will take a lot of time. I am considering combining the base model and adapter as the new base model and do fine-tuning on the new base model. However, currently, when trying to do this, it seems my GPU is not working. I need to do more research on how to realize it."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
